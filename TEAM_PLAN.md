# CS 5542 Lab 5: Snowflake & Streamlit Integration â€” Team Plan

## ðŸŽ¯ **Objective**
Our goal is to take the provided "starter code" (which uses dummy Events/Users data) and **re-engineer it to power our actual Term Project**.

### Project Context: "Academic RAG Pipeline"
Our project involves **Retrieval Augmented Generation (RAG)** for academic papers. We have a dataset of raw PDFs (research papers like *GraphFlow*, *RAGas*) and extracted figures/PNGs.

Instead of trying to upload raw binary PDFs to a SQL table (which doesn't work well for this lab), we will build a **Structured Metadata Layer** in Snowflake that tracks our documents, the extracted text chunks, and the figures.

---

## ðŸ—ï¸ **The New Schema (4 Tables)**
To satisfy the "Star Schema" or "Relational" requirement, we will implement these 4 tables:

1.  **`PAPERS`**: Metadata about the source PDF files (Title, Year, Filename).
2.  **`CHUNKS`**: The processed text segments extracted from the PDFs (Chunk ID, Text Content, Token Count).
3.  **`FIGURES`**: Metadata about the images extracted from the papers (Caption, Filename, Page Num).
4.  **`METRICS`**: Benchmark scores found in the papers (e.g., "Recall@10 = 0.85") for analytics.

---

## ðŸ‘¥ **Role-Based Task Breakdown**

### **Person 1: The Data Engineer ðŸ—ï¸**
**Focus:** Infrastructure and Loading Data.

*   **Core Tasks:**
    *   **Data Prep**: Create the 4 CSV files (`papers.csv`, `chunks.csv`, `figures.csv`, `metrics.csv`) from our raw files. *Strategy: Manually create a small "Golden Set" of 5 papers x 10 chunks to get perfect data for the demo.*
    *   **Schema**: Update `sql/01_create_schema.sql` to define the 4 tables above.
    *   **Pipeline**: Update `scripts/load_local_csv_to_stage.py` to loop through all 4 CSVs and load them into Snowflake automatically.
*   **âœ¨ Extension 1: "Automated Multi-Table Ingestion"**
    *   **Task**: Modify the python script to detect *any* CSV in the `data/` folder and load it into its corresponding table name, rather than hardcoding each one.
    *   **Evidence**: The script output shows "Loaded PAPERS... Loaded CHUNKS... Loaded FIGURES..." sequentially.

---

### **Person 2: The Full-Stack Developer ðŸ’»**
**Focus:** The Dashboard (Streamlit) and Browsing the RAG Knowledge Base.

*   **Core Tasks:**
    *   **"Knowledge Base" Tab**: Create a view in Streamlit to browse papers. Clicking a paper shows its details.
    *   **"Chunk Viewer"**: Create a way to see the text chunks associated with a selected paper.
    *   **"Figure Gallery"**: Display the images (PNGs) associated with a paper. *Note: You can use `st.image` with local file paths if the PNGs are in the repo.*
*   **âœ¨ Extension 2: "Interactive Drill-Down"**
    *   **Task**: Implement a "Master-Detail" interaction.
    *   **Example**: Select a *Metric* (e.g., "Recall") in a dropdown -> Update a chart -> Click a bar in the chart to see the *Papers* that achieved that score.

---

### **Person 3: The Data Analyst ðŸ§ **
**Focus:** SQL Logic and Performance Metrics.

*   **Core Tasks:**
    *   **Query 1 (Aggregation)**: "Average Token Count per Paper".
    *   **Query 2 (Join)**: "List all Figures for Papers published in 2024".
    *   **Query 3 (Complex)**: "Compare Performance Metrics across different Papers" (e.g., Rank papers by Recall@10).
*   **âœ¨ Extension 3: "Materialized View / Analytics"**
    *   **Task**: Create a **View** called `PAPER_SUMMARY` that pre-joins Papers + Metrics + Chunk Counts.
    *   **Why**: This view makes the dashboard faster and easier to query.

---

## ðŸ“… **Execution Plan**

1.  **Data Prep (Immediately)**
    *   **All**: Help create the 4 CSVs. Manually open the PDFs and copy/paste some title/text/metrics into Excel, then Save As CSV.
    *   **Files**: `data/papers.csv`, `data/chunks.csv`, `data/figures.csv`, `data/metrics.csv`.

2.  **Migration (Day 1-2)**
    *   **P1**: Writes the SQL Schema and runs Python Loader. Checks Snowflake to see data.
    *   **P3**: Writes SQL queries against the new tables.
    *   **P2**: Updates `streamlit_app.py` to remove "Events" and add "Papers".

3.  **Extensions (Day 3)**
    *   Implement the 3 specific extensions above.
    *   Record the Demo Video showing the "RAG Knowledge Base" dashboard.

---

## âœ… **Submission Checklist** (Due Feb 23, Noon)

*   [ ] **GitHub Repo** with code + CSVs.
*   [ ] **`pipeline_logs.csv`** (Generated by using the app).
*   [ ] **`CONTRIBUTIONS.md`** (One entry per person).
*   [ ] **Pipeline Diagram** (PDF/PNG).
*   [ ] **Demo Video Link**.
